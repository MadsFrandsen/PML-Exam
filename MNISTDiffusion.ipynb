{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if gpu else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "torch.manual_seed(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device.type == \"cuda:0\" else {}\n",
    "trainset = datasets.MNIST(root='./data', train=True,\n",
    "                          download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size,\n",
    "                          shuffle=True, **kwargs)\n",
    "\n",
    "testset = datasets.MNIST(root='./data', train=False,\n",
    "                          download=True, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(testset, batch_size=batch_size,\n",
    "                         shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    Serves as a convolutional layer in the Diffusion NN\n",
    "\n",
    "    Args:\n",
    "        x: (N, C_in, H, W)\n",
    "    Returns:\n",
    "        y: (N, C_out, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, dilation=1, groups=1, bias=True):\n",
    "        \n",
    "        padding = kernel_size // 2 * dilation\n",
    "        \n",
    "        super().__init__(in_channels, out_channels, kernel_size, padding=padding,\n",
    "                         stride=stride, dilation=dilation, groups=groups, bias=bias)\n",
    "        \n",
    "        self.group_norm = nn.GroupNorm(8, out_channels)\n",
    "        self.activation_fn = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "            x = x + t\n",
    "            temp = x\n",
    "            x = super(ConvLayer, self).forward(x)\n",
    "            x = temp + x\n",
    "            x = self.group_norm(x)\n",
    "            x = self.activation_fn(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionNet(nn.Module):\n",
    "    def __init__(self, T_steps=1000, image_size=[1, 28, 28], hidden_dims=[256, 256], temb_dim=256):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.T=T_steps\n",
    "        self.img_C, self.img_H, self.img_W = image_size\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.temb_dim = temb_dim\n",
    "\n",
    "        self.betas = torch.linspace(0.0001, 0.02, T_steps).to(device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = self.alphas.cumprod(0)\n",
    "\n",
    "        \n",
    "        # input layer\n",
    "        self.in_conv1 = nn.Conv2d(in_channels=self.img_C, out_channels=hidden_dims[0], kernel_size=7, padding=3)\n",
    "        \n",
    "        # time embedding layer\n",
    "        self.t_layer1 = nn.Conv2d(in_channels=temb_dim, out_channels=hidden_dims[0], kernel_size=1, padding=0)\n",
    "        self.t_layer2 = nn.SiLU()\n",
    "        self.t_layer3 = nn.Conv2d(in_channels=hidden_dims[0], out_channels=hidden_dims[0], kernel_size=1, padding=0)\n",
    "        \n",
    "        # middle layers\n",
    "        self.mid_conv1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv_layers = nn.ModuleList([])\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            self.conv_layers.append(ConvLayer(hidden_dims[i-1], hidden_dims[i], kernel_size=3, dilation=3**((i-1)//2)))\n",
    "        \n",
    "        # output layer\n",
    "        self.out_conv1 = nn.Conv2d(in_channels=256, out_channels=self.img_C, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        # embedding\n",
    "        t_embedding = self.pos_encoding(t, self.temb_dim)\n",
    "        t_embedding = self.t_layer2(self.t_layer1(t_embedding.unsqueeze(-1).unsqueeze(-2)))\n",
    "        t_embedding = self.t_layer3(t_embedding)\n",
    "\n",
    "        # input layer\n",
    "        x = self.in_conv1(x)\n",
    "        \n",
    "        # second layer (first mid layer, no activation func)\n",
    "        x = x + t_embedding\n",
    "        temp = x\n",
    "        x = self.mid_conv1(x)\n",
    "        x = temp + x\n",
    "        \n",
    "        # middle layers\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            x = self.conv_layers[i](x, t_embedding)\n",
    "        \n",
    "        # output layer\n",
    "        x = self.out_conv1(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    # scale to -1 to 1, crucial according to DDPM paper.\n",
    "    def minus_one_one(self, x):\n",
    "        return x * 2 - 1\n",
    "    \n",
    "    def zero_one(self, x):\n",
    "        return (x + 1) * 0.5\n",
    "\n",
    "    # function for time embedding, used in the forward part of the NN\n",
    "    def pos_encoding(self, t, channels):\n",
    "        device = t.device\n",
    "        half_dim = channels // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "    \n",
    "    # add noise\n",
    "    def add_noise(self, X, t):\n",
    "        X = self.minus_one_one(X)\n",
    "        epsilon = torch.normal(0., 1., size=X.shape).to(self.device)\n",
    "        \n",
    "        alpha_bar = self.alpha_bars[t][:, None, None, None]\n",
    "        noisy_sample = X * torch.sqrt(alpha_bar) + epsilon * torch.sqrt(1 - alpha_bar)\n",
    "        return noisy_sample\n",
    "        \n",
    "    \n",
    "    # loss function\n",
    "    def loss(self, X):\n",
    "        X = self.minus_one_one(X)\n",
    "        ts = torch.randint(self.T, size=(X.shape[0],)).to(device) # pick t uniformly at random\n",
    "        alpha = self.alpha_bars[ts][:, None, None, None]\n",
    "        epsilon = torch.normal(0., 1., size=X.shape).to(device)\n",
    "        samples = torch.sqrt(alpha)*X + torch.sqrt(1-alpha)*epsilon\n",
    "        eps_model = self.forward(samples, ts)\n",
    "        return nn.functional.mse_loss(eps_model, epsilon)\n",
    "        \n",
    "    # function to generate samples\n",
    "    def sample(self, num):\n",
    "        X = torch.normal(0.0, 1.0, size=(num, self.img_C, self.img_H, self.img_W), dtype=torch.float32).to(device)\n",
    "        \n",
    "        for t in reversed(range(self.T)):\n",
    "            timestep = torch.tensor([t]).repeat(num).to(self.device)\n",
    "\n",
    "            if t > 1:\n",
    "              z = torch.randn_like(X).to(self.device)\n",
    "            else:\n",
    "              z = torch.zeros_like(X).to(self.device)\n",
    "            beta = self.betas[timestep][:, None, None, None]\n",
    "            alpha = self.alphas[timestep][:, None, None, None]\n",
    "            alpha_bar = self.alpha_bars[timestep][:, None, None, None]\n",
    "            eps_model = self.forward(X, timestep)\n",
    "            X = 1 / torch.sqrt(alpha) * (X - (1 - alpha) / torch.sqrt(1 - alpha_bar) * eps_model) + z * torch.sqrt(beta)\n",
    "        X = self.zero_one(X.clamp(-1, 1))\n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n_layers = 8\n",
    "hidden_dim = 256\n",
    "hidden_dims = [hidden_dim for _ in range(n_layers)]\n",
    "\n",
    "num_epochs = 10\n",
    "lr = 5e-5\n",
    "\n",
    "model = DiffusionNet(hidden_dims=hidden_dims).to(device)\n",
    "print(\"Number of model parameters: \", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check if the forward process works!\n",
    "image = images[0]\n",
    "\n",
    "lst = []\n",
    "lst.append(image)\n",
    "for t in range(model.T):\n",
    "    ts = torch.randint(low=t, high=t+1, size=(image.shape[0], )).to(device)\n",
    "    temp = model.add_noise(lst[t].to(device), ts)\n",
    "    temp = model.zero_one(temp)\n",
    "    lst.append(temp)\n",
    "\n",
    "for i in range(100):\n",
    "    if i % 10 == 0:\n",
    "        plt.imshow(lst[i][0].squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "\n",
    "for e in range(1, num_epochs+1):\n",
    "    train_loss = 0.\n",
    "    train_loss_vals = []\n",
    "    for batch_idx, (x, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        l = model.loss(x)\n",
    "        l.backward()\n",
    "        train_loss += l.item()\n",
    "        train_loss_vals.append(train_loss)\n",
    "        optimizer.step()\n",
    "    print(\"\\tEpoch,\", e, \"complete!\", \"\\tLoss: \", train_loss / batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
